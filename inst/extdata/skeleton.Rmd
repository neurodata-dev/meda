---
title: "Matrix Exploratory Data Analysis Output:"
author: "MEDA"
output:
  html_document:
    fig_caption: yes
    fig_height: 8
    fig_width: 8
    echo: FALSE
    highlight: pygments
    number_sections: yes
    theme: cerulean
    toc: yes
---


```{r setup, echo = FALSE}
opts_chunk$set(cache=FALSE, echo=FALSE, fig.height = 8, fig.width= 8,
               warning=FALSE, message=FALSE, fig.show='hold',
               comment="#", fig.keep='low')
```

# Structure
## Column types
```{r cc-str1, results = 'hold'}
print(colStr)
```
## Missing values, NAs, and Negatives
```{r cc-str2, results = 'hold'}
print(paste("Are all rows complete?:", complete))
print(paste("Are there any NAs?:", nas))
print(paste("Are any values negative?:", negs))
```

```{r cc-str3, results = 'hold'}
n <- dim(dat)[1]
d <- dim(dat)[2]
if(d <= 12) { print("Structure"); str(dat) }
```

## Compress data if dimensions are too big.
```{r cc-comp}
if(d > 1e2) { 
  origDat <- as.data.table(dat) 
  dat <- comp(dat, 2, dmethod=dmethod, dnum = 100)
}

if(n > 1e3) { 
  if(!exists("origDat")) origDat <- as.data.table(dat)
  dat <- comp(dat, 1, nmethod=nmethod, nnum = 1e3)
}
```

If $d > 100$ we reduce the number of columns using CUR decomposition. And if $n
> 1e3$ we reduce the number of rows using CUR decoposition.  
Other methods are available. 

# Heatmap
The heatmap below is a representation of the data with values shown in
color according to magnitude. Mouse hover for column names.

[Heatmap](https://en.wikipedia.org/wiki/Heat_map)

```{r cc-heatmap, fig.h = 12}
tryCatch(p.heat(dat))
```

# Location Estimates

The column means and medians are presented in a heatmap below.

```{r cc-locations}
tryCatch(p.location(dat))
```

# Histogram (1D Heatmap)

For each feature column, the data are binned and a heatmap is produced
with each bin colored according to count.

```{r cc-1dheat}
if(dim(dat)[2] < 8){
  tryCatch(p.1dheat(dat) + coord_flip())
  } else {
  tryCatch(p.1dheat(dat))
}
```

# Violin Plot with jittered points

The violin plot combines a kernel density estimate with a boxplot for a
more detailed vizualization.  A jittered scatter plot of the points is overlaid.
The jittering helps reduce effects of overplotting. 

1. [Box plots](https://dx.doi.org/10.2307%2F2683468)
2. [Kernel Density Estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation)
3. [Violin Plots](https://dx.doi.org/10.1080%2F00031305.1998.10480559)

```{r cc-violin}
if(use.plotly){
  tryCatch(ggplotly(p.try(p.violin, dat)))
} else {
  if(dim(dat)[2] < 8){
    p.try(p.violin, dat)
  } else {
    p.try(p.violin, dat) + coord_flip()
  }
}
```


# Correlation Plot

The correlation between two random variables is a measure of a specific
type of dependence that involves not only the two variables themselves
but also a random component.  It measures to what degree a linear
relationship exists between then two random variables, where 1 is
corresponds to a direct linear relationship, 0 corresponds to no linear
relationship, and -1 corresponds to an inverse linear relationship.  

1. [Correlation](https://www.encyclopediaofmath.org/index.php/Correlation_(in_statistics))
2. [Correlation and dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence)
3. [Example graphic](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/506px-Correlation_examples2.svg.png)

```{r cc-corr}
tryCatch(do.call(corrplot, p.cor(dat, colCol = colCol)))
```

# Outlier Plots

An outlier is a datapoint that lives relatively far away from the bulk
of other observations. 
Outliers can have unwanted effects on data analysis
and therefore should be considered carefully. 

We use the built-in method from the `randomForest` package in `R`.

1. [randomForest](https://cran.r-project.org/web/packages/randomForest/index.html)
1. [Outlier](https://en.wikipedia.org/wiki/Outlier)

```{r cc-outlier}
if(use.plotly){
  ggplotly(p.try(p.outlier, dat))
} else {
  p.try(p.outlier, dat)
}
```

# Cumulative Variance 

The variance measure how spread out the data are from their mean.
Cumulative variance measures, as a percentage, how much variation 
each dimension contributes to the dataset. 

In this implementation we use principal components analysis to select
linear combinations of the features that explain the dataset best in
low dimensions. 

The plot below shows how much variance is explained when adding columns
one at a time.  The elbows denote good "cut-off" points for dimension
selection. 

1. [Variance](https://www.encyclopediaofmath.org/index.php/Variance)
2. [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)
3. [Elbows](http://dx.doi.org/10.1016/j.csda.2005.09.010)

```{r cc-cumsum}
if(use.plotly){
  tryCatch(ggplotly(p.try(p.cumvar, dat)))
} else {
  p.try(p.cumvar, dat)
}
```

# Cumulative sum (Correlation Matrix)
```{r cc-Cor-cumsum}
if(use.plotly){
  tryCatch(ggplotly(p.try(p.cumvar, dat)))
} else {
  p.try(p.cumvar, cor(dat))
}
```

# Pairs Plots 
A pairs plot is a popular way of plotting high-dimensional data.  
For every pair of dimensions are plotted showing the specific projection of
the data along those two dimensions. 

For readability a maximum of 8 dimensions are plotted. 

```{r cc-pairs, fig.height = 8, fig.width = 8}
p.try(p.pairs, dat)
```

# BIC Plots 
```{r cc-bic}
out <- p.try(p.bic, dat)
```

# Mclust classifications
```{r cc-mclust}
try(p.mclust(out$data, out$bic, print = ifelse(dim(dat)[1]<100,TRUE,FALSE)))
```

# Binary Hierarhical Mclust classifications
```{r cc-hmclust}
try(p.hmclust(dat, truth = truth))
```

# 3D pca of correlation matrix
```{r cc-3dpca}
try(p.3dpca(dat, colCol = colCol))
```

# Jittered Scatter Plot with classifications
```{r cc-jitter}
if(exists('outHMCclusters')){
  cLab <- outHMCclusters[[length(outHMCclusters)]]$class
} else {
  cLab <- NULL
}

try(p.jitter(dat, clusterLab = cLab))
```




